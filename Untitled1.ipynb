{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('en_vocab_size',20000,\n",
    "                            '''english vocabulary size''')\n",
    "tf.app.flags.DEFINE_integer('en_embedded_size',200,\n",
    "                          '''english embedded size''')\n",
    "tf.app.flags.DEFINE_integer('en_max_length',20,\n",
    "                           '''''')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('zh_vocab_size',10000,\n",
    "                            '''english vocabulary size''')\n",
    "tf.app.flags.DEFINE_integer('zh_embedded_size',200,\n",
    "                          '''english embedded size''')\n",
    "tf.app.flags.DEFINE_integer('zh_max_length',20,\n",
    "                           '''''')\n",
    "tf.app.flags.DEFINE_integer('batch_size',128,\n",
    "                            '''batch size''')\n",
    "\n",
    "tf.app.flags.DEFINE_boolean('is_inference',False,\n",
    "                            '''inference flag''')\n",
    "\n",
    "tf.app.flags.DEFINE_float('learning_rate',0.01,\n",
    "                          '''initial learning rate''')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('decay_step',1000,\n",
    "                            '''decay step''')\n",
    "\n",
    "tf.app.flags.DEFINE_float('decay_rate',0.99,\n",
    "                          '''decay weight''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_integer('attention_size',100,\n",
    "                           '''''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "config = Config()\n",
    "config.encoder_fw_units=[100,80,60]\n",
    "config.encoder_bw_units=[100,80,60]\n",
    "config.out_cell_units=[sum(config.encoder_fw_units)+sum(config.encoder_bw_units),480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(dtype=tf.int32,shape=(FLAGS.batch_size,FLAGS.en_max_length))\n",
    "targets = tf.placeholder(dtype=tf.int32,shape=(FLAGS.batch_size,FLAGS.zh_max_length))\n",
    "start_tokens = tf.placeholder(tf.int32, shape=[], name='start_tokens')\n",
    "end_token = tf.placeholder(tf.int32, shape=[], name='end_token')\n",
    "en_len_sequence = tf.placeholder(dtype=tf.int32, shape=FLAGS.batch_size)\n",
    "zh_len_sequence = tf.placeholder(dtype=tf.int32, shape=FLAGS.batch_size, name='batch_seq_length')\n",
    "embedding_matrix = tf.get_variable(name='embedding_matrix',\n",
    "                                   shape = (FLAGS.en_vocab_size,FLAGS.en_embedded_size),\n",
    "                                   dtype = tf.float32,\n",
    "                                   # regularizer=tf.nn.l2_loss,\n",
    "                                   initializer=tf.truncated_normal_initializer(mean=0,stddev=0.01)\n",
    "                                   )\n",
    "tf.add_to_collection(tf.GraphKeys.LOSSES,tf.nn.l2_loss(embedding_matrix))\n",
    "\n",
    "embedded = tf.nn.embedding_lookup(embedding_matrix,inputs)\n",
    "\n",
    "zh_embedding_matrix = tf.get_variable(name='zh_embedding_matrix',\n",
    "                                      shape=(FLAGS.zh_vocab_size,FLAGS.zh_embedded_size),\n",
    "                                      dtype = tf.float32,\n",
    "                                      # regularizer=tf.nn.l2_loss,\n",
    "                                      initializer=tf.truncated_normal_initializer(mean=0,stddev=0.01))\n",
    "tf.add_to_collection(tf.GraphKeys.LOSSES,tf.nn.l2_loss(zh_embedding_matrix))\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope(\"encoder\"):\n",
    "    cells_fw = [tf.contrib.rnn.GRUCell(num) for num in config.encoder_fw_units ]\n",
    "    cells_bw = [tf.contrib.rnn.GRUCell(num) for num in config.encoder_bw_units ]\n",
    "    outputs,states_fw,states_bw =\\\n",
    "        tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw,\n",
    "                                                       cells_bw,\n",
    "                                                       embedded,\n",
    "                                                       dtype = tf.float32,\n",
    "                                                       sequence_length= en_len_sequence)\n",
    "\n",
    "    dense_fw = tf.concat(states_fw,axis=1)\n",
    "    dense_bw = tf.concat(states_bw,axis=1)\n",
    "    states = tf.concat([dense_bw,dense_fw],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'encoder/concat_2:0' shape=(128, 480) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"decoder\"):\n",
    "    attention_m =\\\n",
    "        tf.contrib.seq2seq.BahdanauAttention(\n",
    "            FLAGS.attention_size,\n",
    "            outputs,\n",
    "            en_len_sequence)\n",
    "    cell_out = [tf.contrib.rnn.GRUCell(num) for num in config.out_cell_units]\n",
    "    cell_attention = \\\n",
    "        [tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell_out[i],attention_m) for i in range(len(config.out_cell_units))]\n",
    "    cells = tf.contrib.rnn.MultiRNNCell(cell_attention)\n",
    "    initial_state = cells.zero_state(dtype=tf.float32,batch_size=FLAGS.batch_size)\n",
    "    initial_state = list(initial_state)\n",
    "    initial_state[0] = initial_state[0].clone(cell_state=states)\n",
    "    initial_state = tuple(initial_state)\n",
    "\n",
    "    if FLAGS.is_inference:\n",
    "\n",
    "        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedded, start_tokens, end_token)\n",
    "    else:\n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            target_embedded = tf.nn.embedding_lookup(zh_embedding_matrix, targets)\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(target_embedded, zh_len_sequence)\n",
    "    dense = Dense(FLAGS.zh_vocab_size)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(cells,helper,initial_state,dense)\n",
    "    logits,final_states,final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "    weights = tf.constant(1.0,shape=[FLAGS.batch_size,FLAGS.zh_max_length])\n",
    "    inference_losses = tf.contrib.seq2seq.sequence_loss(logits.rnn_output,targets,weights)\n",
    "    tf.add_to_collection(tf.GraphKeys.LOSSES,inference_losses)\n",
    "    losses = tf.add_n(tf.get_collection(tf.GraphKeys.LOSSES))\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(FLAGS.learning_rate,\n",
    "                                               global_step,\n",
    "                                               FLAGS.decay_step,\n",
    "                                               FLAGS.decay_rate)\n",
    "\n",
    "\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "    grads_and_vars = opt.compute_gradients(losses)\n",
    "    apply_grads_op = opt.apply_gradients(grads_and_vars,global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder_5/Const:0' shape=(128, 20) dtype=int32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder_7/sequence_loss/truediv:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.add_to_collection(tf.GraphKeys.LOSSES,inference_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'decoder_7/sequence_loss/truediv:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.LOSSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
